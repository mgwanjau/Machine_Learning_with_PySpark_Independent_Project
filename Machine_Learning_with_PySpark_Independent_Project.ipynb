{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYrl-CeLQnXp",
        "outputId": "9e472d8b-c23b-4dd7-9c24-878a785a6e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=77723ae34f75acf09e36aab9477d9ae0fc446dfa727f59aaf82c2322879699de\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the necesary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"TelecomChurnPrediction\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"telecom_dataset.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Handling missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Encoding categorical variables\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\").fit(df) for col in ['Gender', 'Contract', 'Churn']]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Feature columns\n",
        "feature_cols = ['Gender_index', 'Age', 'Contract_index', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "# Assembling the features into a vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Define the models to try\n",
        "models = [\n",
        "    RandomForestClassifier(labelCol=\"Churn_index\", featuresCol=\"features\", seed=42),\n",
        "    LogisticRegression(labelCol=\"Churn_index\", featuresCol=\"features\")\n",
        "]\n",
        "\n",
        "# Create a list of parameter grids to search through\n",
        "paramGrids = [\n",
        "    ParamGridBuilder().addGrid(RandomForestClassifier.maxDepth, [5, 10]).build(),\n",
        "    ParamGridBuilder().addGrid(LogisticRegression.regParam, [0.01, 0.1]).build()\n",
        "]\n",
        "\n",
        "# Create a list to store the accuracy for each model\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate each model\n",
        "for i, model in enumerate(models):\n",
        "    pipeline = Pipeline(stages=[assembler, model])\n",
        "\n",
        "    # Set up the cross-validator\n",
        "    crossval = CrossValidator(estimator=pipeline,\n",
        "                              estimatorParamMaps=paramGrids[i],\n",
        "                              evaluator=BinaryClassificationEvaluator(labelCol=\"Churn_index\"),\n",
        "                              numFolds=5)\n",
        "\n",
        "    # Fit the model and select the best set of parameters\n",
        "    cvModel = crossval.fit(train_data)\n",
        "    bestModel = cvModel.bestModel\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    predictions = bestModel.transform(test_data)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"Churn_index\")\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Print the accuracy for each model\n",
        "    print(f\"Accuracy for Model {i + 1}: {accuracy}\")\n",
        "\n",
        "# Select the best model based on accuracy\n",
        "best_model_index = accuracies.index(max(accuracies))\n",
        "best_model = models[best_model_index]\n",
        "\n",
        "# Train the best model on the full training data\n",
        "pipeline = Pipeline(stages=[assembler, best_model])\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "# Save the best model\n",
        "model.save(\"telecom_churn_model_refined\")\n",
        "\n",
        "# Closing the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "oueWjXS3WSeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}